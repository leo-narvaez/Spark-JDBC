{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc481f71-8af9-402f-92c5-d78236331e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Actividad.\n",
    "La Empresa decide \"migrar\" de sql server a postgreSQL. Efectuar la conexion Databricks Community con PostgreSQL en Azure. Efectuar algunas consultas sobre PostgreSQL usando PySpark y Scala. Utiliza una base de datos cualquiera.  \n",
    "Si la version de community da muchos problemas utilizar Azure Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "039322ae-d018-4862-8ed6-5c9c2fb36789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creación de PosgreSQL en Azure\n",
    "\n",
    "- Crear un grupo de recursos. Luego crear un recurso > Bases de datos > **Azure PostgreSQL**  \n",
    "- Configura los detalles:  \n",
    "  -. Nombre del sesrvidor SQL: `databricks-postgresq-leo` \n",
    "  -. Región: Italy North u otra disponible  \n",
    "  -. Autenticación: Habilita SQL Authentication y configura:  \n",
    "    1. Usuario: `adminuser`  \n",
    "    2. Contraseña: `ContrasenaFuerte123`  \n",
    "- Marca la casilla de \"Habilitar acceso a Azure Services\" \n",
    "- En el portal busca Bases de Datos y crea una con el nombre: `db_postgresql`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb31d33-59ba-485c-98da-fa5066b8e057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La IP pública del nodo es: 34.214.70.55\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " \n",
    "# Obtener la IP pública del nodo\n",
    "public_ip = requests.get('https://api.ipify.org').text\n",
    "print(f\"La IP pública del nodo es: {public_ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fded24-8479-4141-90bf-0bb512d497f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configurar Databricks Community Edition  \n",
    "Ve a la pestaña Computey selecciona Create Compute con estos requisitos:  \n",
    "  - Cluster Name: PSQL_Cluster.  \n",
    "  - Databricks Runtime Version: 11.3 LTS (Scala 2.12, Spark 3.3.1)  \n",
    "  - Crear clúster.  \n",
    "  - Mientras el clúster se esta creando, descarga el controlador JDBC para SQL Server, en este caso usaremos [este](https://jdbc.postgresql.org/download/).  \n",
    "  - En Databricks subir el controlador a tu workspace o a tu DBFS.\n",
    "  - Una vez que el cluster esté activo ve al Cluster y en el boton Libraries cargar el controlador haciendo click en `Install New` y le pasas el path donde has guardado el controlador.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff8cb51-1c27-4602-879c-3d0753765a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de conexión JDBC\n",
    "jdbcHostname = \"databricks-postgresq-leo.postgres.database.azure.com\" # Servidor SQL\n",
    "jdbcPort = 5432\n",
    "jdbcDatabase = \"db_postgresql\" # Nombre exacto de tu base de datos\n",
    "jdbcUsername = \"adminuser\" # Cambiar por tu usuario configurado\n",
    "jdbcPassword = \"ContrasenaFuerte123\" # Cambiar por la contraseña configurada\n",
    "\n",
    "jdbcUrl = f\"jdbc:postgresql://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?sslmode=require\"\n",
    "\n",
    "# Propiedades de conexión\n",
    "connectionProperties = {\n",
    "  \"user\": jdbcUsername,\n",
    "  \"password\": jdbcPassword,\n",
    "  \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20e8ffd-08e2-4b49-be1b-e5a621a5e369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|table_name|\n+----------+\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Intentar realizar la consulta\n",
    "    query = \"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tables\"\n",
    "    df_tables = spark.read.jdbc(url=jdbcUrl, table=query, properties=connectionProperties)\n",
    "    \n",
    "    # Mostrar las tablas\n",
    "    df_tables.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error de conexión:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db7838e8-6a1f-464f-baaf-f21a53859eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 1: Crear la tabla con un DataFrame y escribir en PostgreSQL\n",
    "\n",
    "Aunque PySpark no tiene un método directo para ejecutar comandos como `CREATE TABLE`, puedes crear un DataFrame y escribir los datos directamente a una tabla de PostgreSQL. Si la tabla no existe, PySpark la creará automáticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb79031b-75e5-49ec-88d7-31b3ac32df0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla creada con éxito.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Establecer la conexión con PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=jdbcHostname,\n",
    "    port=jdbcPort,\n",
    "    dbname=jdbcDatabase,\n",
    "    user=jdbcUsername,\n",
    "    password=jdbcPassword\n",
    ")\n",
    "\n",
    "# Crear un cursor para ejecutar la consulta SQL\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Consulta SQL para crear la tabla\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS products (\n",
    "    ProductID SERIAL PRIMARY KEY,\n",
    "    ProductName VARCHAR(100),\n",
    "    Category VARCHAR(50),\n",
    "    Price DECIMAL(10, 2),\n",
    "    StockQuantity INT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Ejecutar la consulta de creación de tabla\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Confirmar los cambios\n",
    "conn.commit()\n",
    "\n",
    "# Cerrar el cursor y la conexión\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Tabla creada con éxito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fdfff04-9305-40e8-99c1-613c10344113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 2: Escribir el DataFrame en PostgreSQL\n",
    "\n",
    "Usando el método `df.write.jdbc()`, puedes escribir los datos del DataFrame en PostgreSQL. Si la tabla no existe, PySpark la crea automáticamente. Si ya existe, puedes optar por reemplazar los datos o agregar nuevos registros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461185b9-4e24-41df-b9d4-d5a8d6e4a3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos insertados con éxito.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Crear datos de ejemplo para los productos\n",
    "product_data = [\n",
    "    Row(ProductName=\"Laptop\", Category=\"Electronics\", Price=799.99, StockQuantity=50),\n",
    "    Row(ProductName=\"Smartphone\", Category=\"Electronics\", Price=499.99, StockQuantity=200),\n",
    "    Row(ProductName=\"Table\", Category=\"Furniture\", Price=150.50, StockQuantity=30),\n",
    "    Row(ProductName=\"Headphones\", Category=\"Electronics\", Price=89.99, StockQuantity=150),\n",
    "    Row(ProductName=\"Coffee Maker\", Category=\"Home Appliances\", Price=120.00, StockQuantity=80)\n",
    "]\n",
    "\n",
    "# Convertir los datos en un DataFrame de PySpark\n",
    "df_products = spark.createDataFrame(product_data)\n",
    "\n",
    "# Insertar los datos en la tabla \"products\" de PostgreSQL\n",
    "df_products.write.jdbc(url=jdbcUrl, table=\"products\", mode=\"append\", properties=connectionProperties)\n",
    "\n",
    "print(\"Datos insertados con éxito.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3581fb11-3f52-46d4-b4d2-07add02db7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 3: Esquema de la tabla\n",
    "\n",
    "PySpark crea la tabla con el esquema definido por el DataFrame. Las columnas del DataFrame serán las mismas que las de la tabla en PostgreSQL, asegurando que el esquema de datos esté alineado entre ambos sistemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be49a6d-7e66-43ce-8a14-b7b3eeea1aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>productid</th><th>productname</th><th>category</th><th>price</th><th>stockquantity</th></tr></thead><tbody><tr><td>1</td><td>Headphones</td><td>Electronics</td><td>89.99</td><td>150</td></tr><tr><td>2</td><td>Laptop</td><td>Electronics</td><td>799.99</td><td>50</td></tr><tr><td>3</td><td>Coffee Maker</td><td>Home Appliances</td><td>120.00</td><td>80</td></tr><tr><td>4</td><td>Table</td><td>Furniture</td><td>150.50</td><td>30</td></tr><tr><td>5</td><td>Smartphone</td><td>Electronics</td><td>499.99</td><td>200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Headphones",
         "Electronics",
         "89.99",
         150
        ],
        [
         2,
         "Laptop",
         "Electronics",
         "799.99",
         50
        ],
        [
         3,
         "Coffee Maker",
         "Home Appliances",
         "120.00",
         80
        ],
        [
         4,
         "Table",
         "Furniture",
         "150.50",
         30
        ],
        [
         5,
         "Smartphone",
         "Electronics",
         "499.99",
         200
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "productid",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "productname",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{\"scale\":2}",
         "name": "price",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "stockquantity",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Leer los datos desde la tabla \"products\" en PostgreSQL\n",
    "df_products_from_db = spark.read.jdbc(url=jdbcUrl, table=\"products\", properties=connectionProperties)\n",
    "\n",
    "# Mostrar los primeros registros\n",
    "df_products_from_db.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38198730-d632-4f03-8df9-f6f95d6d3bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comprobamos, conectandonos en la base de datos por `ssh` y realizamos una consulta.![](files/postgresql/images/query_shell.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d9693cb-6fab-4a1f-b0c7-a3f3d9157179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Constultas SCALA\n",
    "Primero, necesitas definir la configuración de conexión JDBC en Scala de forma similar a como lo hiciste en PySpark. Aquí te muestro cómo hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a286ea19-7493-477e-a935-cd37ef2fab51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">jdbcHostname: String = databricks-postgresq-leo.postgres.database.azure.com\n",
       "jdbcPort: Int = 5432\n",
       "jdbcDatabase: String = db_postgresql\n",
       "jdbcUsername: String = adminuser\n",
       "jdbcPassword: String = ContrasenaFuerte123\n",
       "jdbcUrl: String = jdbc:postgresql://databricks-postgresq-leo.postgres.database.azure.com:5432/db_postgresql?sslmode=require\n",
       "connectionProperties: java.util.Properties = {user=adminuser, password=ContrasenaFuerte123, driver=org.postgresql.Driver}\n",
       "res2: Object = null\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">jdbcHostname: String = databricks-postgresq-leo.postgres.database.azure.com\njdbcPort: Int = 5432\njdbcDatabase: String = db_postgresql\njdbcUsername: String = adminuser\njdbcPassword: String = ContrasenaFuerte123\njdbcUrl: String = jdbc:postgresql://databricks-postgresq-leo.postgres.database.azure.com:5432/db_postgresql?sslmode=require\nconnectionProperties: java.util.Properties = {user=adminuser, password=ContrasenaFuerte123, driver=org.postgresql.Driver}\nres2: Object = null\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Configuración de conexión JDBC\n",
    "val jdbcHostname = \"databricks-postgresq-leo.postgres.database.azure.com\" // Servidor SQL\n",
    "val jdbcPort = 5432\n",
    "val jdbcDatabase = \"db_postgresql\" // Nombre exacto de tu base de datos\n",
    "val jdbcUsername = \"adminuser\" // Cambiar por tu usuario configurado\n",
    "val jdbcPassword = \"ContrasenaFuerte123\" // Cambiar por la contraseña configurada\n",
    "\n",
    "val jdbcUrl = s\"jdbc:postgresql://$jdbcHostname:$jdbcPort/$jdbcDatabase?sslmode=require\"\n",
    "\n",
    "// Propiedades de conexión\n",
    "val connectionProperties = new java.util.Properties()\n",
    "connectionProperties.put(\"user\", jdbcUsername)\n",
    "connectionProperties.put(\"password\", jdbcPassword)\n",
    "connectionProperties.put(\"driver\", \"org.postgresql.Driver\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73b714d-ba97-4443-b165-272727d71ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Seleccionar todos los productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0aad596-f032-44b6-9e02-50417c46fe24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+------------+---------------+------+-------------+\n",
       "productid| productname|       category| price|stockquantity|\n",
       "+---------+------------+---------------+------+-------------+\n",
       "        1|  Headphones|    Electronics| 89.99|          150|\n",
       "        2|      Laptop|    Electronics|799.99|           50|\n",
       "        3|Coffee Maker|Home Appliances|120.00|           80|\n",
       "        4|       Table|      Furniture|150.50|           30|\n",
       "        5|  Smartphone|    Electronics|499.99|          200|\n",
       "+---------+------------+---------------+------+-------------+\n",
       "\n",
       "dfProducts: org.apache.spark.sql.DataFrame = [productid: int, productname: string ... 3 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+------------+---------------+------+-------------+\n|productid| productname|       category| price|stockquantity|\n+---------+------------+---------------+------+-------------+\n|        1|  Headphones|    Electronics| 89.99|          150|\n|        2|      Laptop|    Electronics|799.99|           50|\n|        3|Coffee Maker|Home Appliances|120.00|           80|\n|        4|       Table|      Furniture|150.50|           30|\n|        5|  Smartphone|    Electronics|499.99|          200|\n+---------+------------+---------------+------+-------------+\n\ndfProducts: org.apache.spark.sql.DataFrame = [productid: int, productname: string ... 3 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "dfProducts",
         "schema": {
          "fields": [
           {
            "metadata": {
             "scale": 0
            },
            "name": "productid",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "productname",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 2
            },
            "name": "price",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "stockquantity",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Consultar todos los productos\n",
    "val dfProducts = spark.read.jdbc(jdbcUrl, \"products\", connectionProperties)\n",
    "\n",
    "// Mostrar los primeros 5 productos\n",
    "dfProducts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b957024d-8030-46e6-b077-6c5cae672c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Filtrar productos por categoría \"Electronics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5d20ea-a173-479a-91ac-4ab4fd0f9a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+-----------+-----------+------+-------------+\n",
       "productid|productname|   category| price|stockquantity|\n",
       "+---------+-----------+-----------+------+-------------+\n",
       "        1| Headphones|Electronics| 89.99|          150|\n",
       "        2|     Laptop|Electronics|799.99|           50|\n",
       "        5| Smartphone|Electronics|499.99|          200|\n",
       "+---------+-----------+-----------+------+-------------+\n",
       "\n",
       "dfElectronics: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: int, productname: string ... 3 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+-----------+-----------+------+-------------+\n|productid|productname|   category| price|stockquantity|\n+---------+-----------+-----------+------+-------------+\n|        1| Headphones|Electronics| 89.99|          150|\n|        2|     Laptop|Electronics|799.99|           50|\n|        5| Smartphone|Electronics|499.99|          200|\n+---------+-----------+-----------+------+-------------+\n\ndfElectronics: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: int, productname: string ... 3 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "dfElectronics",
         "schema": {
          "fields": [
           {
            "metadata": {
             "scale": 0
            },
            "name": "productid",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "productname",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 2
            },
            "name": "price",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "stockquantity",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Filtrar productos de la categoría \"Electronics\"\n",
    "val dfElectronics = dfProducts.filter(\"Category = 'Electronics'\")\n",
    "\n",
    "// Mostrar los resultados\n",
    "dfElectronics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d636b140-69a0-4d98-b922-8276a0ed0bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Contar el número de productos por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd3407b-e303-45da-8ef9-a2979a92565a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------------+-----+\n",
       "       Category|count|\n",
       "+---------------+-----+\n",
       "    Electronics|    3|\n",
       "Home Appliances|    1|\n",
       "      Furniture|    1|\n",
       "+---------------+-----+\n",
       "\n",
       "dfCategoryCount: org.apache.spark.sql.DataFrame = [Category: string, count: bigint]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------------+-----+\n|       Category|count|\n+---------------+-----+\n|    Electronics|    3|\n|Home Appliances|    1|\n|      Furniture|    1|\n+---------------+-----+\n\ndfCategoryCount: org.apache.spark.sql.DataFrame = [Category: string, count: bigint]\n</div>",
       "datasetInfos": [
        {
         "name": "dfCategoryCount",
         "schema": {
          "fields": [
           {
            "metadata": {
             "scale": 0
            },
            "name": "Category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "count",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Contar el número de productos por categoría\n",
    "val dfCategoryCount = dfProducts.groupBy(\"Category\").count()\n",
    "\n",
    "// Mostrar el resultado\n",
    "dfCategoryCount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0451c9-9566-4d00-b0a1-684712c8f08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Calcular el precio promedio de los productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9512cc3-3ce5-40fc-8425-bdc3e74cf95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------------+\n",
       "AveragePrice|\n",
       "+------------+\n",
       "  332.094000|\n",
       "+------------+\n",
       "\n",
       "import org.apache.spark.sql.functions._\n",
       "avgPrice: org.apache.spark.sql.DataFrame = [AveragePrice: decimal(14,6)]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------------+\n|AveragePrice|\n+------------+\n|  332.094000|\n+------------+\n\nimport org.apache.spark.sql.functions._\navgPrice: org.apache.spark.sql.DataFrame = [AveragePrice: decimal(14,6)]\n</div>",
       "datasetInfos": [
        {
         "name": "avgPrice",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "AveragePrice",
            "nullable": true,
            "type": "decimal(14,6)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Importar funciones de Spark SQL\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Calcular el precio promedio de los productos\n",
    "val avgPrice = dfProducts.agg(avg(\"Price\").alias(\"AveragePrice\"))\n",
    "\n",
    "// Mostrar el resultado\n",
    "avgPrice.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f40ef307-70d5-4dad-8c63-64b906a6f57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Productos con precio mayor a 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ef589e-548d-4f2e-ac2c-251a71588d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+------------+---------------+------+-------------+\n",
       "productid| productname|       category| price|stockquantity|\n",
       "+---------+------------+---------------+------+-------------+\n",
       "        2|      Laptop|    Electronics|799.99|           50|\n",
       "        3|Coffee Maker|Home Appliances|120.00|           80|\n",
       "        4|       Table|      Furniture|150.50|           30|\n",
       "        5|  Smartphone|    Electronics|499.99|          200|\n",
       "+---------+------------+---------------+------+-------------+\n",
       "\n",
       "dfExpensiveProducts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: int, productname: string ... 3 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+------------+---------------+------+-------------+\n|productid| productname|       category| price|stockquantity|\n+---------+------------+---------------+------+-------------+\n|        2|      Laptop|    Electronics|799.99|           50|\n|        3|Coffee Maker|Home Appliances|120.00|           80|\n|        4|       Table|      Furniture|150.50|           30|\n|        5|  Smartphone|    Electronics|499.99|          200|\n+---------+------------+---------------+------+-------------+\n\ndfExpensiveProducts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: int, productname: string ... 3 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "dfExpensiveProducts",
         "schema": {
          "fields": [
           {
            "metadata": {
             "scale": 0
            },
            "name": "productid",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "productname",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "scale": 2
            },
            "name": "price",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {
             "scale": 0
            },
            "name": "stockquantity",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Filtrar productos con precio mayor a 100\n",
    "val dfExpensiveProducts = dfProducts.filter(\"Price > 100\")\n",
    "\n",
    "// Mostrar los resultados\n",
    "dfExpensiveProducts.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_psql",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
